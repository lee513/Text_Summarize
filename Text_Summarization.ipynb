{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Mecab\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Summarizer():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "        #rank높은 문장 갯수\n",
    "        self.top_n = 3\n",
    "        \n",
    "        ##불용어 사전\n",
    "        stop_words_f =\"./stopwords.txt\"\n",
    "        with open(stop_words_f, \"r\", encoding='utf-8') as f:\n",
    "             stop_words = f.readlines()\n",
    "        self.stop_words = [stop_word.strip() for stop_word in stop_words]\n",
    "        \n",
    "        \n",
    "    #문장 cleaning\n",
    "    def cleaning(self, sentences):\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = re.sub('\\《.*\\》|\\s-\\s.*', '', sentences[i])\n",
    "            sentences[i] = re.sub('\\(.*\\)|\\s-\\s.*', '', sentences[i])\n",
    "            #필드의 태그를 모두 제거\n",
    "            sentences[i] = re.sub('(<([^>]+)>)', '', sentences[i])\n",
    "            # 개행문자 제거\n",
    "            sentences[i] = re.sub('\\n', ' ', sentences[i])\n",
    "            #특수문자 제거\n",
    "            sentences[i] = re.sub(r'[^\\w\\s]', '', sentences[i])\n",
    "        return sentences    \n",
    "    \n",
    "    #mecab으로 조사, 전치사 분리\n",
    "    def mecab_morphs(self, sentences):\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentences[i] = self.mecab.morphs(sentence)\n",
    "        return sentences\n",
    "\n",
    "    #mecab으로 #일반명사 #고유명사 #대명사 #동사 #형용사\n",
    "    def mecab_pos(self, sentences):\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sent_pos = []\n",
    "            for word in self.mecab.pos(sentence):\n",
    "                if word[1] in ['NNG', 'NNP','NP','VV','VA' ]:\n",
    "                    sent_pos.append(word[0])\n",
    "            sentences[i] = sent_pos\n",
    "        return sentences\n",
    "    \n",
    "    def sentence_similarity(self, sent1, sent2):\n",
    "        \n",
    "        #토큰이 비어있으면 제거\n",
    "        for i in range(len(sent1)):\n",
    "            if range(len(sent1[i])) == 0:\n",
    "                sent1.pop(i)\n",
    "        for i in range(len(sent2)):\n",
    "            if range(len(sent2[i])) == 0:\n",
    "                sent2.pop(i)\n",
    "        #비교할 문장들의 토큰합\n",
    "        all_words = list(set(sent1 + sent2))\n",
    "        \n",
    "        #cosine 유사도를 구할 vector 생성\n",
    "        vector1 = [0] * len(all_words)\n",
    "        vector2 = [0] * len(all_words)\n",
    "        \n",
    "        # 첫 문장 생성\n",
    "        for w in sent1:\n",
    "            if w in self.stop_words:\n",
    "                continue\n",
    "            vector1[all_words.index(w)] += 1\n",
    "        # 두번째 문장 생성\n",
    "        for w in sent2:\n",
    "            if w in self.stop_words:\n",
    "                continue\n",
    "            vector2[all_words.index(w)] += 1\n",
    "        \n",
    "        #유사도 계산\n",
    "        return 1 - cosine_distance(vector1, vector2)\n",
    "    \n",
    "\n",
    "    def graph_draw(self):\n",
    "        #각 노드는 각 문장을 의미 엣지의 두께는 연결된 노드들의 코사인 유사도\n",
    "        size = 10\n",
    "        size_similarity_matrix = self.similarity_matrix.flatten()*size\n",
    "        nx.draw(self.G, with_labels=True, node_color='white', width=size_similarity_matrix.tolist())\n",
    "\n",
    "\n",
    "    def __call__(self, document, morphs=False):\n",
    "        \n",
    "        #self.document = document.split(\". \")\n",
    "        #최종 output으로 나올 문서 백업\n",
    "        sentences = document.copy()\n",
    "        \n",
    "        c_sentences = self.cleaning(document)\n",
    "\n",
    "        if morphs == True:\n",
    "            morph_c_sentences = self.mecab_morphs(c_sentences)\n",
    "        else:\n",
    "            morph_c_sentences = self.mecab_pos(c_sentences)\n",
    "        \n",
    "        #유사도행렬 구성\n",
    "        self.similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "        for idx1 in range(len(sentences)):\n",
    "            for idx2 in range(len(sentences)):\n",
    "                if idx1 == idx2: #같은 문장은 계산하지않습니다.\n",
    "                    continue \n",
    "                self.similarity_matrix[idx1][idx2] = self.sentence_similarity(morph_c_sentences[idx1], morph_c_sentences[idx2])\n",
    "    \n",
    "        #유사도행렬을 인접행렬로 취급 무방향 그래프를 그린다.\n",
    "        self.G = nx.from_numpy_array(self.similarity_matrix)\n",
    "        #유사도 행렬로 pagerank 알고리즘 적용\n",
    "        scores = nx.pagerank(self.G)\n",
    "        #랭크 높은순으로 최종 요약 제공\n",
    "        ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "        summarize_text = []\n",
    "        for i in range(self.top_n):\n",
    "          summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
    "    \n",
    "        return \" \".join(summarize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[보안뉴스 문정후 기자] 인공지능이라는 신기술이 기업들의 고객 대응 방법과 사업 운영 방향성을 크게 바꿔놓고 있다',\n",
       " '간단하고 반복적인 임무를 훨씬 정확하고 빠르게 처리해 주고, 수많은 데이터 안에 숨겨져 있어 놓칠 뻔한 통찰을 찾아내 주며, 그러므로 보다 시기적절하고 좋은 결과를 낳는 결정을 내릴 수 있게 해 준다',\n",
       " '그렇지만 인공지능을 주류로 올리기에는 아직까지 해소되지 않은 염려거리들이 있다\\n.인간이 아닌 기계가 데이터를 분석하고 결정까지 내린다고 했을 때 가장 먼저 생각나는 문제점은 ‘윤리성’이다',\n",
       " '기계의 결정이 윤리적으로 그릇된 것일 때가 있다는 건 이미 실제 실험과 프로젝트를 통해 입증되기도 했었다',\n",
       " '그래서 인공지능의 성능과 장점에만 집중하던 기업들은 어느 순간부터 인공지능의 윤리성에도 똑같이 관심을 갖기 시작했다.\\n\\n“인공지능은 강력한 신기술로 장점들만 열거해고 긴 목록이 만들어집니다',\n",
       " '하지만 이 장점들을 빠짐없이 누리기 위해서는 신뢰의 시스템을 먼저 구축할 필요가 있습니다',\n",
       " '인공지능이라는 기술 자체의 신뢰도도 높이고, 인공지능을 개발하는 사람이나 업체의 신뢰도도 높아져야 합니다',\n",
       " '인공지능의 결정을 설명할 수 있어야 하고, 인공지능이 데이터를 어떻게 처리하는지도 투명하게 공개해야 하며, 편견이 반영되지 않도록 하는 작업이 필요합니다.” IBM의 인공지능 윤리 책임자인 프란체스카 로시(Francesca Rossi)의 설명이다.\\n\\n“인공지능의 윤리 문제는 ‘인공지능이 사회에 미칠 영향’이라는 관점에서 이해되어야 합니다',\n",
       " '의도치 않은 사회적 악영향들은 최소화 하면서 전 세계적인 기술 혁신은 이어지도록 해야 하는 것이죠.” 윤리 자문 기업인 에시컬인텔리전스(Ethical Intelligence)의 CEO 올리비아 갬벨린(Olivia Gambelin)의 설명이다',\n",
       " '“인공지능 윤리를 실제 운영 환경에 적용한다는 건 상위 개념의 원리들을 구체적이고 세부적인 행동 지침으로 전환하여 인간 중심의 핵심 가치를 구현한다는 의미가 됩니다.”\\n\\n인공지능의 위험지대\\n인공지능이 잘못 사용되는 사례는 끝도 없이 많다',\n",
       " '미시간대학의 켄타로 토야마(Kentaro Toyama) 교수는 “이미 그런 사례들이 현실로 나타나고 있기도 하다”고 말한다',\n",
       " '“인공지능이 탑재된 군용 드론이 살해 결정을 스스로 내리기도 하고, 딥페이크 기술이 거짓말을 매우 그럴듯하게 만들어 퍼트리기도 하죠',\n",
       " '인공지능이 특정 인종이나 성별을 차별한 사례들은 널리 알려져 있습니다',\n",
       " '인공지능을 학습시키기 위한 데이터가 그 어떤 편견에도 물들어 있지 않은지, 우리는 어떻게 확인할 수 있을까요? 어려운 문제입니다.”\\n\\n이런 상태에서 인공지능 기술의 애플리케이션 개발에 속도가 붙었다',\n",
       " '너도 나도 인공지능이라는 수식어를 붙여 상품을 시장에 내놓기 시작한 것이다',\n",
       " '데이터 분석 업체 피코(FICO)의 분석관인 스콧 졸디(Scott Zoldi)는 “그 결과 인공지능이 스스로 온갖 차별을 저지르면서 특정 인종이나 부류의 사람들이 피해를 입게 되었다”고 말한다',\n",
       " '“더 심각한 건 인공지능 알고리즘이 하는 결정과, 그 결정으로 인해 야기된 결과가 어떻게 도출되었고 어떤 영향을 사회에 미치는지를 점검하려 하지 않는다는 겁니다',\n",
       " '반성이 없는 기업들의 태도가 인공지능 기술에 대한 신뢰만 겹겹이 쌓고 있습니다.”\\n\\n인텔(Intel)의 인공지능 연구원인 라마 나크만(Lama Nachman)은 “인공지능이 여러 번 학습한 형태의 데이터에 대해서는 꽤나 괜찮은 결과를 내는데, 전혀 새로운 형태의 새로운 데이터가 주입되면 엉뚱한 결과를 내는 경우가 많다”고 지적한다',\n",
       " '“이 때문에 인공지능을 사람의 감독이나 관리 없이 그냥 사용하는 데에 주저할 수밖에 없습니다',\n",
       " '대표적인 게 자율주행 분야죠',\n",
       " '거리에서 수도 없이 생기는 사건사고의 변수들을 인공지능이 독자적으로 처리할 수 있을 거라는 자신감이 아직 전문가들에게나 소비자들에게 생기지 않고 있습니다',\n",
       " '실험을 하는 것도 대단히 위험하고요.”\\n\\n인공지능 윤리 문제 처리하기\\n이렇게 인공지능을 불안하게 만드는 많은 요인들 중 하나가 ‘인공지능 윤리’다',\n",
       " '반대로 아직 인공지능이 성숙해지기 전 ‘인공지능 윤리’ 문제가 확립되는 것이 나으므로, 지금 시기에 이 문제가 불거진 게 오히려 다행이라는 시각도 존재한다',\n",
       " '나크만은 인공지능 윤리라는 것에 대해 “인공지능 기술을 실제 구축할 때 따라야 할 원칙과 가이드라인의 일종”이라고 간략하게 설명한다.\\n\\n“윤리라고 해서 대단한 인문학적 철학을 알고리즘화 하는, 그런 게 아닙니다',\n",
       " '기업의 리스크 분석 접근법에 기반을 두고 인공지능이 어떤 식으로 구축되고, 어떤 과정을 거쳐 어떤 결과를 내며, 어떤 식으로 상호작용을 했으면 좋겠다고 결정하는 것입니다',\n",
       " '공평하고, 투명하며, 프라이버시를 보호하는 것 등이 요즘 모두 비즈니스 리스크에 포함이 되는 것처럼 인공지능 활용에도 포함이 되는 것일 뿐이죠.”\\n\\n갬벨린은 “각종 인공지능 관련 정책과 규정들이 신설되고 있기 때문에 인공지능 윤리 정책이라는 것도 곧 표준화가 될 것”이라고 내다보고 있다',\n",
       " '“그렇다면 ‘인공지능 윤리’라는 것이 규정 준수의 일부에 포함될 수도 있겠죠',\n",
       " '다만 지금의 인공지능 윤리보다 훨씬 복잡하고 엄격한 것이 될 것이라고 예상할 수도 있을 겁니다',\n",
       " '대신 이걸 지켰을 때 아낄 수 있는 시간과 비용은 결코 무시할 수준의 것이 되지 못할 거라고 봅니다',\n",
       " '오히려 그런 규정과 표준들이 정립됨으로써 인공지능 기술 혁신이 가속화 될 거라고 개인적으로 믿고 있습니다.”\\n\\n인공지능 윤리, 앞으로 어떻게 될까\\n그렇기 때문에 인공지능 기술들을 개발하는 조직들이라면 시작부터 인공지능 윤리 문제를 같이 고민해야 한다고 PwC의 글로벌 인공지능 담당인 아난드 라오(Anand Rao)는 강조한다',\n",
       " '“인공지능 윤리 문제가 제대로 반영되지 알고리즘을, 출시 후에 되돌린다는 건 굉장히 복잡하고 어려운 일이 됩니다',\n",
       " '소 잃고 외양간 고치는 격이 될 가능성도 높고요',\n",
       " '일종의 가이드라인으로서 먼저 항목들을 정의하고, 그 틀 안에서 인공지능을 개발하는 게 안전하고 빠른 방법입니다.”\\n\\n2021년 PwC 연구 조사에 의하면 인공지능 윤리 프레임워크를 갖춘 기업들은 20%도 되지 않는다고 한다',\n",
       " '인공지능 기반 시스템이나 프로세스의 거버넌스를 향상시킬 계획을 가지고 있는 기업도 35% 이하였다',\n",
       " '“다만 ‘적절한 인공지능의 결정’이라는 결과를 내는 데 투자하겠다는 기업들이 대다수였습니다',\n",
       " '인공지능 개발 그 자체가 목적이 아니라, 인공지능이 내리는 결정에도 책임을 져야 한다는 의식이 어느 정도 퍼지고 있다는 것으로 해석됩니다',\n",
       " '그나마 긍정적인 부분이죠.” 라오의 설명이다.\\n\\n로시는 “인공지능 기술에 대한 신뢰를 구축하려면 다양한 분야와 영역의 사람들이 참여하여 통합적인 접근체계를 마련해야 한다”고 강조한다',\n",
       " '“완전무결한 인공지능 애플리케이션을 처음부터 만들어낼 수는 없습니다',\n",
       " '분명히 어느 지점에서는 이상하고, 문제가 될 만한 오류가 나올 수 있습니다',\n",
       " 'IT 기술 개발 인력이 상상도 못할 것들 말이죠',\n",
       " '그러니 모두가 각자의 방법으로 인공지능을 점검하고 실험하여 문제를 미리 파악하고, 협조적으로 해결해야 합니다',\n",
       " '인공지능은 이제 IT 분야가 아니라, ‘전 분야’를 아우르는 기술이 되었습니다.”']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference\n",
    "file_name = \"test1.txt\"\n",
    "file = open(file_name, \"r\", encoding=\"UTF-8\")\n",
    "document = file.read()\n",
    "article = document.split(\". \")\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능의 결정을 설명할 수 있어야 하고, 인공지능이 데이터를 어떻게 처리하는지도 투명하게 공개해야 하며, 편견이 반영되지 않도록 하는 작업이 필요합니다.” IBM의 인공지능 윤리 책임자인 프란체스카 로시(Francesca Rossi)의 설명이다.\n",
      "\n",
      "“인공지능의 윤리 문제는 ‘인공지능이 사회에 미칠 영향’이라는 관점에서 이해되어야 합니다 오히려 그런 규정과 표준들이 정립됨으로써 인공지능 기술 혁신이 가속화 될 거라고 개인적으로 믿고 있습니다.”\n",
      "\n",
      "인공지능 윤리, 앞으로 어떻게 될까\n",
      "그렇기 때문에 인공지능 기술들을 개발하는 조직들이라면 시작부터 인공지능 윤리 문제를 같이 고민해야 한다고 PwC의 글로벌 인공지능 담당인 아난드 라오(Anand Rao)는 강조한다 실험을 하는 것도 대단히 위험하고요.”\n",
      "\n",
      "인공지능 윤리 문제 처리하기\n",
      "이렇게 인공지능을 불안하게 만드는 많은 요인들 중 하나가 ‘인공지능 윤리’다\n"
     ]
    }
   ],
   "source": [
    "text_summarize = Text_Summarizer()\n",
    "summarize = text_summarize(article)\n",
    "print(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference\n",
    "file_name = \"test1.txt\"\n",
    "file = open(file_name, \"r\", encoding=\"UTF-8\")\n",
    "document = file.read()\n",
    "article = document.split(\". \")\n",
    "\n",
    "\n",
    "text_summarize.graph_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사설 문서 요약 검증\n",
    "def validation_result(morphs = False):\n",
    "    val_df = pd.read_pickle(\"val_df.pkl\")\n",
    "\n",
    "    references = []\n",
    "    for i in range(len(val_df.extractive)):\n",
    "        val = \" \".join(val_df.extractive[i])\n",
    "        references.append(val)\n",
    "\n",
    "    #검증 데이터 모델 추론\n",
    "    text_sumarize = Text_Summarizer()\n",
    "    inferences = []\n",
    "    for i in range(len(val_df.text)):\n",
    "        result = text_sumarize(val_df.text[i], morphs=morphs)\n",
    "        inferences.append(result)\n",
    "        print(\"{a}경과중\".format(a=i))\n",
    "\n",
    "    #추론 리스트와 정답 리스트의 길이 확인\n",
    "    print(f'inferences_length: {len(inferences)}, references_length: {len(references)}')\n",
    "\n",
    "    return inferences, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences, references = validation_result(morphs = False)\n",
    "scores = rouge.get_scores(inferences, references, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_2_valdata_score.pkl','rb') as f:\n",
    "    scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rouge-1: recall: 0.337 precision: 0.335 f-1: 0.332 \n",
      " Rouge-2: recall: 0.288 precision: 0.280 f-1: 0.281 \n",
      " Rouge-L: recall: 0.336 precision: 0.333 f-1: 0.330\n"
     ]
    }
   ],
   "source": [
    "print(\" Rouge-1: recall: {:.3f} precision: {:.3f} f-1: {:.3f}\".format(scores['rouge-1']['r'], scores['rouge-1']['p'], scores['rouge-1']['f']),'\\n',\n",
    "      \"Rouge-2: recall: {:.3f} precision: {:.3f} f-1: {:.3f}\".format(scores['rouge-2']['r'], scores['rouge-2']['p'], scores['rouge-2']['f']),'\\n',\n",
    "      \"Rouge-L: recall: {:.3f} precision: {:.3f} f-1: {:.3f}\".format(scores['rouge-l']['r'], scores['rouge-l']['p'], scores['rouge-l']['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f39af212ce94a7b0cdd9317a6168b02f9a85e854a440d9053cc7b257b88fe43e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
